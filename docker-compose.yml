services:
  # Astral Discord Bot
  astral-bot:
    build: ./bot
    container_name: astral-bot
    env_file: .env
    environment:
      # Use host Ollama (models already downloaded locally)
      - OLLAMA_HOST=http://host.docker.internal:11434
      - TABBY_MODEL=${TABBY_MODEL}
      - TABBY_HOST=${TABBY_HOST}
      - TABBY_API_KEY=${TABBY_API_KEY}
      - KOBOLD_HOST=${KOBOLD_HOST}
      - KOBOLD_MODEL=${KOBOLD_MODEL}
      - LLM_BACKEND=${LLM_BACKEND:-tabby}
      - SEARXNG_HOST=http://searxng:8080
      - RAG_DATABASE=/app/data/db/memory.db
      - CHARACTERS_FILE=/app/data/characters.json
      - ASSETS_DIR=/app/data/assets
      - KOKORO_TTS_URL=http://host.docker.internal:8000
      - PYTHONUNBUFFERED=1 # Enable real-time logging
    depends_on:
      searxng:
        condition: service_started
    volumes:
      - ./bot:/app # Live code mount (overlay entire app for hot reload)
      - ./db:/app/data/db # Persist RAG database
    restart: unless-stopped
    networks:
      - astral-network
    # Allow access to host network on Windows Docker
    extra_hosts:
      - "host.docker.internal:host-gateway"

  # KoboldCpp - Local GGUF Inference (OpenAI-compatible API)
  koboldcpp:
    image: koboldai/koboldcpp:latest
    container_name: astral-koboldcpp
    ports:
      - "5001:5001"
    volumes:
      - ./koboldcpp/models:/workspace/models:ro
      - ./koboldcpp:/workspace/config:ro
    environment:
      - KCPP_MODEL=/workspace/models/${KOBOLD_MODEL_FILE:-GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-Q4_K_S.gguf}
      - KCPP_ARGS=--usecuda mmq --gpulayers 99 --contextsize ${KOBOLD_CONTEXT:-8192} --chatcompletionsadapter /workspace/config/${KOBOLD_ADAPTER:-glm47-nothink-adapter.json} --port 5001 --host 0.0.0.0
      - KCPP_DONT_TUNNEL=true
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    networks:
      - astral-network

  # SearXNG - Free Unlimited Search
  searxng:
    image: searxng/searxng:latest
    container_name: astral-searxng
    ports:
      - "8080:8080"
    volumes:
      - ./searxng:/etc/searxng:rw
    environment:
      - SEARXNG_BASE_URL=http://localhost:8080
    restart: unless-stopped
    networks:
      - astral-network

networks:
  astral-network:
    driver: bridge
